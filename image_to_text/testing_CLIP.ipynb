{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willrathgeb/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 4.52k/4.52k [00:00<00:00, 1.16MB/s]\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "Downloading model.safetensors: 100%|██████████| 1.71G/1.71G [07:38<00:00, 3.73MB/s]\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 316/316 [00:00<00:00, 86.0kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 905/905 [00:00<00:00, 519kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 961k/961k [00:00<00:00, 1.29MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 525k/525k [00:00<00:00, 1.25MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.22M/2.22M [00:00<00:00, 3.48MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 389/389 [00:00<00:00, 293kB/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willrathgeb/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open('model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processor.pkl', 'rb') as processor_file:\n",
    "    processor = pickle.load(processor_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = loaded_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPOutput(loss=None, logits_per_image=tensor([[18.9041, 11.7159]], grad_fn=<TBackward0>), logits_per_text=tensor([[18.9041],\n",
       "        [11.7159]], grad_fn=<MulBackward0>), text_embeds=tensor([[-0.0031,  0.0028,  0.0291,  ...,  0.0227,  0.0088,  0.0132],\n",
       "        [ 0.0091, -0.0039,  0.0203,  ...,  0.0310,  0.0025,  0.0109]],\n",
       "       grad_fn=<DivBackward0>), image_embeds=tensor([[-3.1594e-02,  7.3143e-02, -6.0848e-04,  8.8355e-03,  3.3088e-02,\n",
       "          8.4753e-03, -1.3071e-02, -6.0082e-03,  2.4028e-03,  7.9934e-03,\n",
       "         -6.5093e-02, -2.8512e-02,  1.8172e-02,  3.8944e-02, -1.5353e-02,\n",
       "          2.9794e-02, -9.1005e-03,  7.0502e-04, -6.8631e-03,  2.6676e-03,\n",
       "         -2.6455e-02, -2.7599e-02,  8.9061e-03,  2.3567e-03, -4.8174e-02,\n",
       "          8.6455e-05,  2.0355e-02,  1.1162e-02,  1.2210e-02,  1.2164e-02,\n",
       "          1.4173e-02,  1.9187e-02,  1.5167e-02,  4.0698e-03, -4.5474e-02,\n",
       "          9.9183e-03, -7.3702e-03, -1.2674e-03, -4.2590e-02,  8.4714e-03,\n",
       "          3.9972e-02,  3.5804e-02, -1.4753e-02,  5.6062e-03,  1.1241e-02,\n",
       "          1.8721e-02,  2.8468e-02,  8.9770e-03, -4.3563e-03, -1.3921e-02,\n",
       "         -2.4466e-02, -3.8155e-04, -6.0559e-03,  2.1481e-02,  1.7916e-02,\n",
       "         -1.4489e-02, -4.8004e-04, -2.1082e-03,  7.6649e-03,  1.1985e-02,\n",
       "          3.3106e-02, -4.9733e-02,  7.0811e-04,  2.5579e-02,  1.7403e-02,\n",
       "         -2.7512e-02,  6.4442e-03, -7.3650e-04,  5.8902e-03, -5.0638e-03,\n",
       "          2.0092e-02, -4.0540e-02,  1.8965e-02, -3.0864e-02,  8.3066e-04,\n",
       "         -7.9286e-03,  2.0762e-03, -4.6186e-02, -7.0521e-03,  1.1845e-03,\n",
       "          6.5650e-03, -5.8188e-03,  2.5765e-02, -1.7675e-02, -3.5840e-02,\n",
       "         -3.2164e-02, -4.5415e-02, -2.3243e-02,  1.3475e-02,  3.4238e-02,\n",
       "         -3.8686e-02,  3.4016e-02, -1.3186e-02,  1.0938e-02,  2.5678e-02,\n",
       "          4.5763e-02,  5.7385e-02, -1.6503e-02,  4.3591e-02,  1.3097e-02,\n",
       "         -3.5419e-02,  2.6613e-02,  1.2353e-02,  2.5906e-02, -2.3537e-02,\n",
       "         -1.7222e-02, -1.3133e-02, -3.3629e-02, -1.5209e-02, -5.0772e-02,\n",
       "          2.1341e-02,  5.1715e-03,  7.1198e-03,  9.8945e-03, -3.5178e-02,\n",
       "         -5.1742e-03,  6.3006e-03, -1.0370e-02, -1.2276e-02, -3.7246e-02,\n",
       "          1.3445e-02, -2.3314e-02, -2.3382e-02, -1.1742e-02, -9.5012e-03,\n",
       "         -5.4354e-02,  3.7149e-02,  3.3962e-03,  2.0530e-02, -7.4955e-03,\n",
       "         -7.0453e-03,  3.2278e-02, -1.1497e-03, -3.1096e-02,  2.6525e-02,\n",
       "          1.2749e-02,  2.3580e-02,  2.5054e-02, -2.9208e-02,  2.0238e-02,\n",
       "          5.1917e-03, -2.7924e-02, -4.9504e-02, -3.9070e-04, -7.6092e-02,\n",
       "          5.0480e-02,  1.1126e-02, -3.8628e-02, -1.2888e-02,  1.0364e-02,\n",
       "         -1.0281e-02,  4.0417e-02, -2.9155e-02,  3.9344e-02, -2.1643e-02,\n",
       "          4.5990e-02,  6.1562e-03,  5.8455e-02, -7.0963e-03, -2.0204e-02,\n",
       "          2.4640e-02, -7.3316e-03, -2.2939e-02,  1.3955e-02,  4.1070e-03,\n",
       "         -2.4667e-02, -3.4989e-02, -5.2653e-03, -2.1736e-02,  8.9419e-04,\n",
       "          7.9459e-03, -1.8826e-02, -2.6818e-02,  8.9893e-03,  8.6416e-03,\n",
       "          3.0746e-02,  1.0647e-02, -1.2346e-02,  1.8410e-02, -1.3082e-03,\n",
       "          9.7276e-02, -9.9856e-03, -3.3905e-02, -4.5365e-02, -1.7048e-02,\n",
       "         -2.7603e-02,  9.9468e-02, -1.5182e-03,  6.4525e-03,  2.0820e-03,\n",
       "         -3.3420e-03,  3.0685e-04,  1.5341e-03,  1.9517e-03, -1.0133e-02,\n",
       "          2.7802e-02,  1.7530e-02,  1.2842e-02, -2.6736e-02,  1.1640e-02,\n",
       "          2.1499e-02,  2.0341e-02,  1.3428e-02, -3.3864e-02,  6.2468e-03,\n",
       "         -6.3106e-03,  2.2791e-02,  1.7214e-03, -3.3639e-03, -2.5367e-02,\n",
       "          2.8813e-02,  1.9597e-02, -3.0934e-02,  2.7195e-02, -2.5208e-02,\n",
       "          3.7077e-02,  5.9417e-02,  4.0762e-02, -1.2774e-02, -1.4100e-04,\n",
       "         -2.1948e-02, -8.5129e-04,  1.1314e-02,  4.2418e-02, -6.0840e-04,\n",
       "         -2.9395e-03, -8.7036e-03,  3.4352e-02,  3.4045e-03,  7.1651e-03,\n",
       "         -2.5966e-03, -1.5818e-02,  2.2355e-02,  3.1454e-02,  4.0121e-02,\n",
       "          3.4628e-03, -3.5618e-02,  2.0595e-02,  3.6926e-02,  8.0931e-03,\n",
       "         -2.9701e-02, -2.0267e-02, -2.5762e-02,  1.9988e-03, -2.1680e-02,\n",
       "          6.7399e-03, -3.2109e-02,  5.1960e-02, -1.9453e-02,  9.7369e-04,\n",
       "         -2.2269e-02,  2.0203e-02, -2.7423e-02, -3.6623e-02,  2.9017e-02,\n",
       "         -9.0881e-06,  6.5010e-03,  4.8463e-02,  3.4247e-02, -4.2163e-03,\n",
       "          1.8434e-02,  1.9428e-02, -2.1527e-02, -8.5871e-03,  1.2481e-02,\n",
       "          1.8621e-03, -3.1823e-02, -8.6497e-03,  1.9468e-02,  1.1500e-03,\n",
       "          3.2793e-02,  2.8982e-03, -3.3732e-02, -1.5349e-02,  3.4730e-02,\n",
       "          1.2286e-02,  1.0350e-02,  4.2773e-03,  1.0658e-02, -2.2952e-02,\n",
       "         -1.7384e-03, -9.4330e-02, -1.0510e-02, -8.0578e-02,  1.2847e-02,\n",
       "         -1.2355e-03,  1.5886e-02,  1.7072e-02,  1.1498e-02,  2.3484e-02,\n",
       "         -7.6426e-03, -2.6608e-03, -1.4018e-02,  1.2347e-02,  2.5744e-02,\n",
       "         -1.8626e-02,  1.9969e-02, -3.3191e-02,  2.3385e-02,  4.9538e-02,\n",
       "         -2.6342e-02,  3.1884e-02,  1.5555e-02, -4.0097e-03, -2.2281e-02,\n",
       "          1.9865e-02,  2.9782e-02,  4.0520e-02,  1.6797e-02,  3.3104e-03,\n",
       "          6.1624e-03,  7.7413e-03, -4.3241e-02, -5.3853e-02,  1.0415e-02,\n",
       "          5.8274e-04, -7.3373e-02, -7.2072e-02,  4.4669e-04,  1.4264e-02,\n",
       "          6.5389e-03,  3.2042e-03, -7.3628e-03, -1.1014e-02,  2.3360e-02,\n",
       "          3.2330e-02,  3.6752e-02,  1.6277e-03,  1.2627e-02, -1.1252e-02,\n",
       "          3.9758e-02,  5.0580e-02, -8.5665e-03, -7.6229e-04, -1.1295e-02,\n",
       "          1.5338e-02,  1.0715e-02, -3.3042e-03, -1.2896e-02,  1.7243e-02,\n",
       "          3.6709e-04, -3.2321e-02, -5.7882e-03, -2.9191e-02, -3.0872e-02,\n",
       "         -2.2492e-04, -1.3690e-03,  2.8289e-03,  3.3493e-02, -6.8209e-03,\n",
       "          2.2719e-02,  2.3291e-02, -1.2719e-03, -2.2178e-02, -4.7872e-02,\n",
       "          2.7172e-02,  3.0428e-04,  1.1272e-03, -2.1091e-02,  1.7848e-03,\n",
       "          8.1873e-03,  2.5819e-02, -1.0532e-03, -2.1740e-02,  1.9511e-03,\n",
       "          2.9494e-02,  1.5277e-02, -6.7677e-02,  3.8119e-03,  1.7717e-02,\n",
       "          3.6312e-02, -4.2543e-02, -2.6372e-02,  9.0403e-03,  2.7583e-02,\n",
       "         -5.3353e-02, -2.7474e-03, -5.6964e-02, -3.0712e-02,  8.0514e-03,\n",
       "          4.2383e-02,  8.4143e-03,  2.4426e-02,  6.0953e-03,  1.1809e-03,\n",
       "          5.5165e-02, -3.3703e-02, -6.0831e-03,  7.8391e-03, -1.2550e-02,\n",
       "         -5.3179e-02,  4.7374e-03,  1.0683e-02, -9.6379e-03,  1.9863e-02,\n",
       "         -1.6002e-02, -1.2871e-02, -4.2597e-03,  2.3073e-02,  1.8429e-02,\n",
       "          6.6298e-03, -1.7984e-02,  2.4378e-02, -3.5589e-02, -5.3075e-02,\n",
       "         -3.4583e-02,  1.5231e-03, -3.8575e-02,  3.1016e-03, -1.8447e-02,\n",
       "         -4.3316e-02, -1.3634e-02,  1.0900e-02, -4.4885e-01,  1.5686e-02,\n",
       "          1.0339e-02,  4.6067e-03,  7.3741e-02,  3.6903e-03, -4.6897e-03,\n",
       "          3.5621e-03, -1.0757e-02, -6.4115e-03,  3.0094e-02, -6.8155e-03,\n",
       "         -1.0099e-02, -8.4084e-03,  2.0410e-02,  5.9516e-03,  7.3553e-03,\n",
       "          2.8187e-02,  1.8237e-02, -1.6029e-02,  2.8039e-02,  5.3214e-02,\n",
       "          1.1083e-01, -3.0069e-04,  2.0781e-02,  2.0782e-02,  3.3038e-02,\n",
       "          6.1236e-02,  9.7231e-03,  2.2376e-02,  2.6198e-02, -3.1137e-02,\n",
       "         -1.3464e-02,  9.4015e-03,  5.7376e-03,  6.5690e-04,  1.5401e-02,\n",
       "         -5.4072e-02, -3.4217e-02, -2.3013e-02,  4.4904e-02,  6.0023e-03,\n",
       "          2.9391e-03, -4.5314e-03, -2.4535e-02, -4.6659e-03,  3.3743e-03,\n",
       "          2.5481e-02, -1.4173e-02,  9.3307e-03,  1.4152e-01, -1.3918e-02,\n",
       "          1.1636e-02, -1.8263e-03,  4.7347e-02, -6.1782e-03,  3.7584e-02,\n",
       "          1.0647e-03,  2.4787e-03,  2.1986e-02, -1.2089e-02, -1.3593e-02,\n",
       "          2.3821e-02,  2.5133e-02, -2.8683e-03,  1.8588e-03, -2.2728e-02,\n",
       "          2.6455e-03, -1.1094e-02, -2.6051e-02, -2.9490e-02,  2.5770e-02,\n",
       "          2.6804e-03,  1.1978e-04, -1.4806e-02, -4.6252e-02,  2.4960e-02,\n",
       "         -2.9269e-02,  3.2147e-02, -1.2887e-02,  1.5465e-03,  3.1087e-02,\n",
       "         -3.3309e-02,  6.0908e-02, -1.7476e-02, -2.8730e-02, -1.4807e-03,\n",
       "         -9.6841e-03, -9.1644e-03, -4.9799e-02, -1.2693e-02,  7.5168e-03,\n",
       "         -7.4462e-03, -2.7937e-02, -2.1875e-02, -6.0949e-04, -4.0124e-03,\n",
       "          3.0791e-02,  1.6025e-02, -9.2272e-03, -3.3007e-03,  5.1472e-05,\n",
       "          3.2686e-02, -2.7117e-02,  1.5845e-02,  2.8578e-03,  1.2663e-02,\n",
       "          1.7274e-02, -1.5356e-02,  2.2047e-02, -2.1921e-02, -8.6464e-03,\n",
       "          1.1817e-02, -1.3897e-02,  3.9782e-03,  3.5045e-02, -1.9475e-02,\n",
       "         -8.8259e-03, -1.5545e-03,  3.3621e-02, -3.3691e-02,  3.6791e-02,\n",
       "          1.9536e-03, -1.0808e-02, -7.2295e-04, -4.6083e-02, -2.9483e-02,\n",
       "         -2.9616e-02, -1.2670e-02, -1.6431e-04,  9.8972e-03, -5.4079e-02,\n",
       "         -5.4280e-02,  5.6100e-03,  3.5348e-03,  2.1702e-02,  2.4056e-02,\n",
       "         -7.5780e-02,  6.9264e-02,  1.7879e-02,  8.1619e-03,  1.9017e-03,\n",
       "         -1.5757e-02, -1.2863e-02,  2.3128e-02,  2.3781e-02, -3.1077e-02,\n",
       "          1.3875e-01,  2.6850e-02, -1.2141e-02,  3.2050e-02, -7.7507e-02,\n",
       "         -2.4027e-02,  1.6555e-03, -2.7325e-03, -3.9161e-02,  2.5328e-02,\n",
       "          3.2035e-02,  4.2793e-02,  1.1515e-02, -9.5609e-03,  2.3921e-04,\n",
       "          1.9599e-02,  1.2818e-02,  8.7465e-03,  1.2943e-03, -2.3948e-02,\n",
       "          1.6104e-02,  2.6292e-02, -1.2102e-02,  6.7069e-03,  8.5057e-03,\n",
       "         -8.5877e-03, -4.2545e-03, -3.1043e-02, -1.7689e-02,  7.8872e-03,\n",
       "         -3.4053e-03, -3.9125e-02,  1.1464e-02, -1.4457e-02,  2.8908e-02,\n",
       "          3.4043e-02, -7.8630e-03, -1.9600e-02,  1.5263e-03, -2.5012e-02,\n",
       "         -4.2395e-03,  2.2118e-03, -6.9534e-03,  4.6416e-02, -2.3147e-02,\n",
       "         -1.1616e-01, -4.0813e-02, -6.6223e-03, -4.0658e-02,  2.9167e-02,\n",
       "          6.6188e-03, -1.7927e-02, -3.9096e-03, -2.7031e-03, -5.2506e-03,\n",
       "         -4.7673e-03,  2.9670e-02,  1.2227e-03,  6.7270e-02, -1.5234e-02,\n",
       "          4.2697e-02,  8.0637e-03,  1.7937e-02,  2.8018e-02,  4.0434e-02,\n",
       "         -6.7446e-03, -5.6958e-04, -2.3822e-03,  1.0871e-02, -1.1388e-02,\n",
       "         -2.5824e-02,  2.5135e-02,  4.3797e-03,  3.5918e-02, -1.0794e-02,\n",
       "          8.6512e-02, -2.6198e-02, -2.6901e-02,  7.7006e-03,  1.2901e-02,\n",
       "         -9.3563e-03, -1.2439e-02,  3.5309e-03,  2.3589e-02,  4.8006e-01,\n",
       "         -1.9519e-02, -5.9958e-03, -8.0213e-03,  1.2932e-02,  2.6459e-02,\n",
       "         -1.0971e-02, -5.0866e-02, -3.2936e-02, -3.4898e-02,  3.3433e-03,\n",
       "         -3.5985e-02,  1.2972e-02,  2.2943e-02,  6.6313e-03,  5.1901e-02,\n",
       "         -5.1910e-02, -1.6529e-02, -4.1744e-02, -2.2921e-02,  1.1155e-03,\n",
       "         -2.2640e-02,  6.4307e-03,  1.0166e-02,  1.0053e-02,  1.7882e-02,\n",
       "          2.9117e-03, -3.5842e-03, -1.2211e-02, -5.4163e-02,  8.1244e-03,\n",
       "         -8.7753e-03, -6.0105e-03,  3.0906e-02, -1.7902e-03,  2.6545e-02,\n",
       "          7.2275e-03, -1.9290e-02,  5.2553e-03,  2.6408e-02,  5.3849e-03,\n",
       "          2.7049e-02, -1.5461e-02,  2.6013e-02, -1.1575e-02, -2.9970e-02,\n",
       "         -1.3677e-02,  4.3330e-03,  4.4033e-02,  2.1993e-02,  1.6632e-02,\n",
       "         -1.6682e-02, -1.3869e-02,  2.8647e-03, -1.1097e-02, -2.0379e-02,\n",
       "         -8.6570e-03, -1.1626e-02,  2.5312e-02,  4.2398e-02, -1.8522e-02,\n",
       "         -3.7229e-03, -1.2763e-02, -3.8205e-03,  7.0125e-02,  3.0947e-03,\n",
       "         -1.5072e-02,  9.3923e-04, -2.0701e-02, -6.9147e-03,  2.9308e-02,\n",
       "         -3.5177e-02,  2.2513e-02,  1.6114e-02,  1.6239e-02,  8.3135e-03,\n",
       "         -8.3226e-03, -7.0733e-03, -3.0811e-02,  1.1680e-02, -7.8671e-02,\n",
       "         -1.3418e-02,  7.9883e-03,  1.5208e-02, -1.8516e-02, -1.5661e-02,\n",
       "         -1.4979e-02, -8.2909e-03,  7.5793e-04, -2.1981e-02, -9.7378e-03,\n",
       "          1.7318e-02, -2.0592e-02, -2.2093e-02, -1.4671e-02, -2.8802e-02,\n",
       "          2.9555e-03, -4.0982e-02,  7.8116e-03,  1.0134e-02, -1.1990e-02,\n",
       "         -1.4194e-02,  2.4258e-02, -2.0680e-02,  3.1344e-02, -2.3676e-02,\n",
       "          4.4455e-03,  3.3145e-02,  2.4230e-02,  5.8416e-03,  4.9578e-02,\n",
       "         -3.2810e-02,  5.6528e-03, -2.5444e-02,  3.0413e-02,  3.8130e-04,\n",
       "          1.3244e-02,  5.0576e-03,  8.8226e-03, -1.1385e-02,  4.0256e-02,\n",
       "         -5.1002e-02,  5.3214e-04, -2.6412e-03]], grad_fn=<DivBackward0>), text_model_output=BaseModelOutputWithPooling(last_hidden_state=tensor([[[-3.8838e-01,  2.2944e-02, -5.2197e-02,  ..., -4.8988e-01,\n",
       "          -3.0660e-01,  6.7454e-02],\n",
       "         [ 2.9033e-02, -1.3258e+00,  3.0846e-01,  ..., -5.2568e-01,\n",
       "           9.7681e-01,  6.6517e-01],\n",
       "         [ 1.1565e+00,  1.3184e-01,  7.8949e-01,  ..., -2.1024e+00,\n",
       "          -1.1519e+00, -3.3113e-01],\n",
       "         ...,\n",
       "         [ 5.8302e-01, -1.3717e-01,  2.1518e+00,  ..., -1.0520e+00,\n",
       "          -1.5292e-01,  9.6756e-02],\n",
       "         [-7.8088e-02,  9.8267e-01,  6.9147e-01,  ..., -2.8869e+00,\n",
       "           2.0970e-02, -4.1268e-01],\n",
       "         [-1.2166e+00, -5.1482e-01,  4.8002e-01,  ..., -1.3832e-01,\n",
       "           8.1141e-01,  5.5702e-01]],\n",
       "\n",
       "        [[-3.8838e-01,  2.2944e-02, -5.2197e-02,  ..., -4.8988e-01,\n",
       "          -3.0660e-01,  6.7454e-02],\n",
       "         [ 2.9033e-02, -1.3258e+00,  3.0846e-01,  ..., -5.2568e-01,\n",
       "           9.7681e-01,  6.6517e-01],\n",
       "         [ 1.1565e+00,  1.3184e-01,  7.8949e-01,  ..., -2.1024e+00,\n",
       "          -1.1519e+00, -3.3113e-01],\n",
       "         ...,\n",
       "         [ 5.8302e-01, -1.3717e-01,  2.1518e+00,  ..., -1.0520e+00,\n",
       "          -1.5292e-01,  9.6756e-02],\n",
       "         [-1.7998e+00,  7.1469e-01,  9.7601e-01,  ..., -1.3804e+00,\n",
       "          -9.1295e-01,  1.2958e-02],\n",
       "         [-9.5111e-01,  7.5061e-03,  1.3867e-01,  ...,  6.0344e-04,\n",
       "           2.2237e-01,  8.9224e-01]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-1.2166e+00, -5.1482e-01,  4.8002e-01,  ..., -1.3832e-01,\n",
       "          8.1141e-01,  5.5702e-01],\n",
       "        [-9.5111e-01,  7.5061e-03,  1.3867e-01,  ...,  6.0344e-04,\n",
       "          2.2237e-01,  8.9224e-01]], grad_fn=<IndexBackward0>), hidden_states=None, attentions=None), vision_model_output=BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.5297, -0.7713,  0.4655,  ..., -0.3993, -0.0721, -0.3703],\n",
       "         [ 0.8688,  0.1690,  0.6678,  ...,  0.5126, -1.1465, -0.1258],\n",
       "         [ 1.1742, -0.7551,  0.0396,  ...,  0.7166, -0.5458,  0.0031],\n",
       "         ...,\n",
       "         [ 0.8636,  0.2223,  0.6411,  ...,  0.5242, -0.8104,  0.0170],\n",
       "         [ 0.6842, -1.1056, -0.2486,  ...,  0.7901,  0.4862, -0.0949],\n",
       "         [ 0.8934,  0.0066,  0.9235,  ...,  0.5707, -0.8436, -0.2182]]],\n",
       "       grad_fn=<AddBackward0>), pooler_output=tensor([[-0.9326, -1.3289,  0.7919,  ..., -0.3337, -0.0479, -0.7106]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_per_image = outputs.logits_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18.9041, 11.7159]], grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = logits_per_image.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CLIPModel' object has no attribute 'encode_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     image_features \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39;49mencode_image(image)\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CLIPModel' object has no attribute 'encode_image'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = loaded_model.encode_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model, preprocess \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mViT-B/32\u001b[39;49m\u001b[39m\"\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/clip/clip.py:139\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, device, jit, download_root)\u001b[0m\n\u001b[1;32m    136\u001b[0m         state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(opened_file, map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m jit:\n\u001b[0;32m--> 139\u001b[0m     model \u001b[39m=\u001b[39m build_model(state_dict \u001b[39mor\u001b[39;49;00m model\u001b[39m.\u001b[39;49mstate_dict())\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(device) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    141\u001b[0m         model\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/clip/model.py:424\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(state_dict)\u001b[0m\n\u001b[1;32m    421\u001b[0m transformer_heads \u001b[39m=\u001b[39m transformer_width \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m    422\u001b[0m transformer_layers \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(k\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m state_dict \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mtransformer.resblocks\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m--> 424\u001b[0m model \u001b[39m=\u001b[39m CLIP(\n\u001b[1;32m    425\u001b[0m     embed_dim,\n\u001b[1;32m    426\u001b[0m     image_resolution, vision_layers, vision_width, vision_patch_size,\n\u001b[1;32m    427\u001b[0m     context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n\u001b[1;32m    428\u001b[0m )\n\u001b[1;32m    430\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39minput_resolution\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontext_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvocab_size\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    431\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/clip/model.py:290\u001b[0m, in \u001b[0;36mCLIP.__init__\u001b[0;34m(self, embed_dim, image_resolution, vision_layers, vision_width, vision_patch_size, context_length, vocab_size, transformer_width, transformer_heads, transformer_layers)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m Transformer(\n\u001b[1;32m    283\u001b[0m     width\u001b[39m=\u001b[39mtransformer_width,\n\u001b[1;32m    284\u001b[0m     layers\u001b[39m=\u001b[39mtransformer_layers,\n\u001b[1;32m    285\u001b[0m     heads\u001b[39m=\u001b[39mtransformer_heads,\n\u001b[1;32m    286\u001b[0m     attn_mask\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_attention_mask()\n\u001b[1;32m    287\u001b[0m )\n\u001b[1;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m vocab_size\n\u001b[0;32m--> 290\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mEmbedding(vocab_size, transformer_width)\n\u001b[1;32m    291\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mempty(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_length, transformer_width))\n\u001b[1;32m    292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final \u001b[39m=\u001b[39m LayerNorm(transformer_width)\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/torch/nn/modules/sparse.py:144\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m _weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs),\n\u001b[1;32m    143\u001b[0m                             requires_grad\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m _freeze)\n\u001b[0;32m--> 144\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n\u001b[1;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlist\u001b[39m(_weight\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[1;32m    147\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/torch/nn/modules/sparse.py:153\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     init\u001b[39m.\u001b[39;49mnormal_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill_padding_idx_with_zero()\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/torch/nn/init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[39m=\u001b[39mtensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd)\n\u001b[0;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/torch/nn/init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[1;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mnormal_(mean, std)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.1397e-01, -1.4601e-01,  3.0359e-01, -1.9986e-01,  1.4242e-02,\n",
       "         -1.0353e-01,  1.5506e-01,  2.9515e-01,  4.7935e-01, -7.2346e-03,\n",
       "         -3.1607e-01,  1.3016e-01, -1.3686e-01,  5.4776e-01,  2.1390e-01,\n",
       "         -3.0370e-01, -1.5691e-01,  5.4069e-01, -4.2241e-01,  1.9465e-01,\n",
       "          2.2643e-02, -1.2344e-01,  1.5223e-01,  8.3010e-02, -5.9309e-01,\n",
       "          5.9104e-01, -1.4420e-01,  2.0842e-01,  2.5099e-03, -2.7938e-01,\n",
       "          2.5470e-01,  2.3694e-01, -1.5010e-01,  7.6442e-01,  2.6988e-01,\n",
       "          2.9660e-02,  1.5769e-01,  4.3022e-01,  1.0241e-02, -2.1385e+00,\n",
       "         -6.2538e-02,  3.6795e-01, -2.6137e-01, -3.4203e-01, -1.7093e-01,\n",
       "         -7.1249e-01, -3.4283e-01, -5.0471e-02, -3.2371e-01, -1.2000e-01,\n",
       "          6.4516e-02, -2.5436e-01, -1.0889e-01,  5.8420e-01,  7.9974e-02,\n",
       "         -2.9603e-01, -2.2696e-02,  4.7601e-01, -9.7581e-02,  5.6397e-01,\n",
       "          6.9377e-01, -3.1860e-02,  2.3535e-01, -8.8878e-02,  2.1019e-02,\n",
       "          6.5319e-02,  2.6316e-01,  7.1715e-01, -2.8215e-01, -1.7884e-01,\n",
       "         -1.9458e-01, -2.6096e-01, -1.3835e-01,  3.9474e-01,  2.1236e-01,\n",
       "         -5.2514e-01,  9.4686e-02, -5.4848e-02, -1.5926e-01, -3.5243e-01,\n",
       "         -3.5027e-01,  3.1223e-01, -3.7849e-01,  5.9759e-01,  4.6590e-02,\n",
       "          4.2470e-01,  1.2218e+00,  1.8287e-01,  5.6845e-01, -3.0903e-01,\n",
       "          3.1819e-01, -4.0420e-02, -5.8692e+00,  4.5910e-01,  1.3757e-01,\n",
       "         -2.2637e-02,  5.1162e-02, -3.0350e-01, -1.7293e-01, -3.0445e-02,\n",
       "         -1.4706e-01, -3.5252e-03, -3.0881e-01, -2.2078e-01,  6.5432e-02,\n",
       "         -3.6447e-01, -3.4441e+00,  5.3642e-01,  7.5141e-02,  7.4901e-03,\n",
       "          7.6060e-02, -5.1893e-01, -4.1571e-02, -1.3567e-01, -3.8545e-01,\n",
       "          2.2007e-01,  1.0523e-01, -7.7671e-01,  2.9042e-01,  2.9057e-01,\n",
       "         -7.7224e-02, -2.4166e-01, -9.7721e-02,  9.8533e-02,  1.6275e-01,\n",
       "          2.7338e-01,  3.2891e-02, -7.2356e-02,  2.4436e-01,  2.6715e-01,\n",
       "          6.3879e-01,  6.1584e-02,  4.3011e-02,  8.1464e-01, -3.6322e-02,\n",
       "         -3.5045e-02, -3.1839e-01, -7.6682e-01, -1.2731e-01,  1.7144e-01,\n",
       "          3.8453e-01,  4.1469e-03, -5.1209e-01, -3.0298e-01, -2.1678e-01,\n",
       "          5.1554e-01, -1.7220e-01,  1.9053e-01, -1.0610e-01, -2.3382e-01,\n",
       "          8.7415e-02,  2.4527e-01, -1.5732e-01, -6.2306e-02,  2.7762e-02,\n",
       "          8.0385e-02, -2.8613e-01, -2.7654e-01,  8.4961e-02, -1.9044e-02,\n",
       "          6.7762e-01, -1.8661e-01,  1.5328e-01, -4.0721e-01,  4.8336e-01,\n",
       "         -2.0414e-01, -7.9041e-03,  1.0186e-01, -6.0048e-02, -7.1651e-02,\n",
       "         -8.1219e-02, -1.2353e-01, -7.7829e-02, -4.4665e-01, -2.3897e-01,\n",
       "          3.8857e-01, -7.5676e-01,  4.3817e-01, -4.4356e-02,  4.3902e-01,\n",
       "         -1.3020e-01, -6.5705e-01, -4.8938e-01,  1.3553e-02, -1.1848e-01,\n",
       "         -3.0796e-01,  1.7875e-01,  4.4655e-01, -3.6892e-01, -1.1394e-01,\n",
       "         -1.6277e-01, -5.8749e-02,  3.4434e-02, -4.0822e-03, -5.1869e-01,\n",
       "         -1.8889e-01,  1.2889e-01,  4.1027e-01, -2.6452e-02,  1.1224e-02,\n",
       "          2.6184e-01, -1.7111e-01, -7.2165e-03, -6.0798e-01,  2.7803e-01,\n",
       "         -4.9015e-02, -3.6089e-01,  1.4578e-01,  1.8531e-01,  1.1282e-01,\n",
       "         -1.5344e-01, -5.0823e-01, -2.0847e-02,  1.6738e-01, -6.1899e-02,\n",
       "         -1.8441e-01,  3.9025e-01,  4.6567e-01,  4.6110e-02, -2.7818e-02,\n",
       "         -4.4871e-03,  6.1488e-01,  1.2431e-01,  4.6309e-02, -2.1120e-01,\n",
       "         -3.9603e-01,  4.5331e-01, -1.5113e-01, -7.0637e-02,  3.5263e-01,\n",
       "         -1.6702e-01,  4.2283e-01, -2.6500e-02,  3.4193e-01, -3.0356e-01,\n",
       "         -3.7544e-02, -8.5392e-02, -4.3938e-03, -2.8398e-01, -1.5064e-01,\n",
       "         -4.0772e-01, -1.8841e-01,  2.3547e-01,  3.8282e-01, -1.7383e-01,\n",
       "          7.5951e-01,  2.6151e-01,  2.1454e-01,  1.8997e-01,  3.6280e-01,\n",
       "          4.3264e-02, -3.8931e-02, -1.4569e-01,  1.0715e-01, -1.6045e-01,\n",
       "          4.9624e-01, -3.5086e-01, -7.9697e-02, -1.2396e+00,  9.4297e-02,\n",
       "         -2.3942e-01,  1.1775e-02,  2.3101e-01,  5.3898e-01, -4.5457e-01,\n",
       "          3.4575e-03, -3.7836e-01, -6.6983e-01, -1.4715e-01,  8.4177e-02,\n",
       "         -3.0629e-01, -1.1741e-01, -1.0529e-01,  2.4088e-01,  9.8099e-02,\n",
       "          3.8931e-01, -5.8134e-02,  2.3119e-01, -3.3540e-02,  5.4438e-01,\n",
       "          2.9337e-01, -2.0390e-01, -3.5354e-01, -8.5205e-03,  3.9849e-02,\n",
       "          3.1603e-01, -8.8790e-02, -4.4542e-02,  2.2270e-02, -1.8894e-01,\n",
       "          3.6508e-01,  7.6460e-02, -1.1224e-01,  8.3123e-02,  2.7721e-01,\n",
       "          2.7787e-01,  2.7032e-02, -4.9543e-01, -1.1674e-01,  1.3514e-03,\n",
       "          3.0114e-01, -9.3956e-02, -4.0411e-01,  3.0524e-01,  5.2388e-03,\n",
       "         -2.8184e-01,  3.4177e-01,  1.5813e-01,  7.0803e-01,  5.6982e-01,\n",
       "         -6.5738e-01, -5.2924e-02,  8.1389e-01,  1.7868e-01, -3.2224e-01,\n",
       "         -2.6354e-01,  2.1508e-01,  1.7309e-01,  8.8935e-02,  6.6937e-02,\n",
       "          7.5425e-01,  1.7436e+00, -1.6382e-02,  3.5861e-01, -7.8767e-01,\n",
       "         -2.7695e-01, -4.0547e-01, -3.0609e-01,  5.6199e-02,  9.0761e-03,\n",
       "          3.0741e-01,  5.6072e-01,  2.5087e-01, -3.3014e-01, -1.7515e-01,\n",
       "          1.8643e-01,  8.1083e-02,  2.5515e-01, -7.1167e-02,  1.6110e-01,\n",
       "          2.5221e-02, -5.4908e-01,  2.8034e-01, -2.5767e-02, -8.4296e-02,\n",
       "         -1.8107e-01,  4.4544e-02, -1.4849e-02,  1.9754e-01,  2.6326e-01,\n",
       "         -8.7926e-02, -2.7375e-01,  1.4462e-03,  2.0529e-02, -1.4149e-01,\n",
       "          1.3711e-01,  2.4432e-01, -6.3533e-01, -2.5031e-01,  6.2312e-01,\n",
       "         -1.6525e-01, -1.5219e-01,  2.6615e-01, -2.6544e-01, -8.2765e-01,\n",
       "          4.8787e-01,  3.5338e-02, -8.3093e-01,  2.4473e-01,  3.8722e-01,\n",
       "         -3.1210e-01, -1.9773e-01,  4.9135e-01,  9.5394e-02, -5.3649e-02,\n",
       "          6.7343e-03, -8.0504e-02,  1.0593e-01, -4.2588e-01, -3.1446e-01,\n",
       "         -1.3326e-01, -3.1450e-01, -2.8545e-01,  1.0698e-01,  1.2888e-02,\n",
       "         -9.5468e-02,  8.1598e-02, -5.7026e-01,  3.7035e-01,  1.0319e+00,\n",
       "         -5.1504e-01,  3.6989e-01,  1.8606e-01, -6.4591e-02,  3.1360e-01,\n",
       "          5.6157e-02, -2.0666e-01, -5.1312e-01, -4.4908e-01, -1.5091e-01,\n",
       "          3.6242e-01,  1.0888e-01, -2.1584e-01,  5.7332e-02,  1.3463e-01,\n",
       "         -1.6298e-01, -4.7442e-01, -1.1092e-02,  8.8110e-01, -1.8848e-01,\n",
       "          2.4846e-01, -3.3838e-01, -1.3492e-02,  3.6865e-01,  2.3043e-01,\n",
       "         -1.1222e-01,  5.5824e-01, -2.4656e-01, -2.8021e-01, -1.1543e-01,\n",
       "         -1.8866e-01, -4.6277e-02,  3.0954e-01, -4.1389e-01, -3.5432e-01,\n",
       "          8.7788e-02,  1.0957e-01, -3.2369e-01, -1.4049e+00, -1.2743e-01,\n",
       "          1.1550e-01,  1.7964e-01, -3.9975e-02, -4.5162e-02, -1.5717e-01,\n",
       "         -2.7980e-01, -2.2724e-01, -2.8938e-01, -1.4991e-01, -3.0734e-01,\n",
       "         -2.0658e-01,  1.5952e-01, -4.1966e-01,  7.0709e-02,  3.8448e-01,\n",
       "         -4.3484e-01, -1.2617e-01,  1.1510e-01,  1.0302e-01,  5.8868e-01,\n",
       "         -8.5872e-01, -3.8338e-01,  4.6080e-01, -3.8657e-01, -1.8431e-01,\n",
       "          3.6574e-02, -7.1071e-02,  8.1448e-02, -2.1004e-02,  9.6193e-01,\n",
       "          4.5979e-01, -2.5262e-01,  4.9560e-01, -2.6480e-01, -5.6205e-01,\n",
       "          1.6420e-01,  1.9973e-02, -5.3964e-02,  4.4655e-02, -2.1840e-01,\n",
       "          3.8857e-01,  3.4692e-01, -1.3216e-01, -2.1852e-01,  5.4724e-02,\n",
       "          1.3728e-01,  2.4407e-01,  2.4325e-01,  3.0561e-01,  7.0876e-02,\n",
       "          5.9802e-02, -3.6059e-01, -2.1450e-01,  1.4299e-01,  2.5721e-01,\n",
       "         -2.6258e-02, -1.9067e-01,  3.2603e-01,  7.8368e-03,  5.9975e-02,\n",
       "          2.2920e-01, -1.8104e-02,  2.8932e-01, -4.5101e-01,  6.8298e-02,\n",
       "          5.7297e-02,  2.9609e-01, -8.5148e-02, -2.5641e-02, -2.7756e-01,\n",
       "         -5.5894e-01, -3.4894e-01, -3.4013e-01,  5.2391e-01, -9.5474e-03,\n",
       "          1.4748e-02, -2.6559e-01,  7.0068e-02, -4.2351e-01,  8.8749e-01,\n",
       "         -2.5461e-01, -1.1299e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lavis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/willrathgeb/CalHacks2023/calHacks2023/image_to_text/testing_CLIP.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/image_to_text/testing_CLIP.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mclip_text_decoder\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageCaptionInferenceModel\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/clip_text_decoder/model.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor, nn, optim\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m GPT2Tokenizer\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mclip_text_decoder\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     check_language_model,\n\u001b[1;32m     17\u001b[0m     check_vision_backbone,\n\u001b[1;32m     18\u001b[0m     encode_image_tensor,\n\u001b[1;32m     19\u001b[0m     load_language_model,\n\u001b[1;32m     20\u001b[0m     load_vision_backbone,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m PRETRAINED_INFERENCE_MODEL_PATH \u001b[39m=\u001b[39m (\n\u001b[1;32m     24\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhttps://drive.google.com/uc?id=1bEAyV2279C4V4iYMaJahREiM58vjy6G1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m     \u001b[39m# https://drive.google.com/file/d/1bEAyV2279C4V4iYMaJahREiM58vjy6G1/view?usp=sharing\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDecoder\u001b[39;00m(LightningModule):\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/clip_text_decoder/common.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mclip\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m CLIP\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlavis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m BlipFeatureExtractor, load_model_and_preprocess\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor, nn\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lavis'"
     ]
    }
   ],
   "source": [
    "from clip_text_decoder.model import ImageCaptionInferenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clip_text_decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mclip_text_decoder\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageCaptionInferenceModel\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb#X46sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcurrent_milli_time\u001b[39m():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/testing_CLIP.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mround\u001b[39m(time\u001b[39m.\u001b[39mtime() \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clip_text_decoder'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from clip_text_decoder.model import ImageCaptionInferenceModel\n",
    "\n",
    "\n",
    "def current_milli_time():\n",
    "    return round(time.time() * 1000)\n",
    "\n",
    "files = os.listdir(\"images/\")\n",
    "\n",
    "\n",
    "model = ImageCaptionInferenceModel.download_pretrained(\"clip/model.pt\")\n",
    "model.to(\"cpu\")\n",
    "\n",
    "start = current_milli_time()\n",
    "for file in files:\n",
    "    image_path = \"images/\" + file\n",
    "    image = Image.open(image_path)\n",
    "    jpg_image = image.convert('RGB')\n",
    "    # if img.shape > 3:\n",
    "    #     image = images[0]\n",
    "    # The beam_size argument is optional. Larger beam_size is slower, but has\n",
    "    # slightly higher accuracy. Recommend using beam_size <= 3.\n",
    "    caption = model(jpg_image, beam_size=1)\n",
    "    print(file[:-4] + \": \" + caption)\n",
    "    print(\"Time elapsed: \" + str(int((current_milli_time()-start)/1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "# Pickle the model\n",
    "with open('processor.pkl', 'wb') as processor_file:\n",
    "    pickle.dump(processor, processor_file)\n",
    "\n",
    "\n",
    "# Unpickle the model\n",
    "#with open('image_to_text/processor.pkl', 'rb') as processor_file:\n",
    "#    loaded_model = pickle.load(processor_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|██████████| 445/445 [00:00<00:00, 123kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 527/527 [00:00<00:00, 936kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 911kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 829kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 251kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 4.60k/4.60k [00:00<00:00, 3.29MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.88G/1.88G [02:16<00:00, 13.7MB/s]\n",
      "/Users/willrathgeb/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photography of a woman and her dog on the beach\n",
      "woman sitting on the beach with her dog and a cell phone\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willrathgeb/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat is sitting on the beach with a woman and a dog\n"
     ]
    }
   ],
   "source": [
    "text = \"The cat is\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman sitting on the beach with her dog and a cell phone\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_img_url = 'https://www.hydroflask.com/media/catalog/product/W/3/W32BTS001-Black-StraightOn.jpg' \n",
    "wb_raw_image = Image.open(requests.get(wb_img_url, stream=True).raw).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a detailed description of the item in this picture is a hydro flask\n"
     ]
    }
   ],
   "source": [
    "text = \"A detailed description of the item in this picture is \"\n",
    "inputs = processor(wb_raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this picture is a close up of a hydro flask bottle\n"
     ]
    }
   ],
   "source": [
    "text = \"This picture is \"\n",
    "inputs = processor(wb_raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this picture is an image of a hydro flask bottle\n"
     ]
    }
   ],
   "source": [
    "text = \"This picture is an image of \"\n",
    "inputs = processor(wb_raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hydro flask bottle is black with a black lid\n"
     ]
    }
   ],
   "source": [
    "text = \"The hydro flask bottle is \"\n",
    "inputs = processor(wb_raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hydro flask bottle is black with a black lid\n"
     ]
    }
   ],
   "source": [
    "text = \"The hydro flask bottle is \"\n",
    "inputs = processor(wb_raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hydro flask bottle is black with a black lid\n"
     ]
    }
   ],
   "source": [
    "text = \"The hydro flask bottle is \"\n",
    "inputs = processor(wb_raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hydro flask bottle is black with a black lid and a black handle\n"
     ]
    }
   ],
   "source": [
    "text = \"The hydro flask bottle is black with a black lid and \"\n",
    "inputs = processor(wb_raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hydro flask bottle is black with a black lid and a black handle and a black lid\n"
     ]
    }
   ],
   "source": [
    "text = \"the hydro flask bottle is black with a black lid and a black handle and \"\n",
    "inputs = processor(wb_raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
