{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groundingdino.datasets.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.getcwd()\n",
    "# set model configuration file path\n",
    "# CONFIG_PATH = os.path.join(HOME, \"groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "\n",
    "# set model weight file ath\n",
    "WEIGHTS_PATH = 'GroundingDINO/weights/groundingdino_swint_ogc.pth'\n",
    "\n",
    "# set text prompt\n",
    "TEXT_PROMPT = \"glass with lid\"\n",
    "\n",
    "# set box and text threshold values\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willrathgeb/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"GroundingDINO/weights/groundingdino_swint_ogc.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willrathgeb/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:905: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/Users/willrathgeb/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/Users/willrathgeb/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m AWB \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Face recognition and opencv setup\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mVideoCapture(URL \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m:81/stream\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ret, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# create a transform function by applying 3 image transaformations\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "while True:\n",
    "     # ESP32 URL\n",
    "    URL = 'http://172.20.10.6'\n",
    "    AWB = True\n",
    "\n",
    "    # Face recognition and opencv setup\n",
    "    cap = cv2.VideoCapture(URL + \":81/stream\")\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    # create a transform function by applying 3 image transaformations\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )\n",
    "    # convert frame to a PIL object in RGB space\n",
    "    image_source = Image.fromarray(frame).convert(\"RGB\")\n",
    "    # convert the PIL image object to a transform object\n",
    "    image_transformed, _ = transform(image_source, None)\n",
    "    \n",
    "    image_source.save('test.jpg')\n",
    "\n",
    "    # predict boxes, logits, phrases\n",
    "    boxes, logits, phrases = predict(\n",
    "    model=model, \n",
    "    image=image_transformed, \n",
    "    caption=TEXT_PROMPT, \n",
    "    box_threshold=BOX_TRESHOLD, \n",
    "    text_threshold=TEXT_TRESHOLD,\n",
    "    device='cpu')\n",
    "    \n",
    "    # annotate the image\n",
    "    annotated_frame = annotate(image_source=frame, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    # display the output\n",
    "    out_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imshow('frame', out_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'weights/dog-3.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m BOX_TRESHOLD \u001b[39m=\u001b[39m \u001b[39m0.35\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m TEXT_TRESHOLD \u001b[39m=\u001b[39m \u001b[39m0.25\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m image_source, image \u001b[39m=\u001b[39m load_image(IMAGE_PATH)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m boxes, logits, phrases \u001b[39m=\u001b[39m predict(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     image\u001b[39m=\u001b[39mimage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     text_threshold\u001b[39m=\u001b[39mTEXT_TRESHOLD\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willrathgeb/CalHacks2023/calHacks2023/dino.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m annotated_frame \u001b[39m=\u001b[39m annotate(image_source\u001b[39m=\u001b[39mimage_source, boxes\u001b[39m=\u001b[39mboxes, logits\u001b[39m=\u001b[39mlogits, phrases\u001b[39m=\u001b[39mphrases)\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/groundingdino/util/inference.py:46\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_image\u001b[39m(image_path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39marray, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m     39\u001b[0m     transform \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mCompose(\n\u001b[1;32m     40\u001b[0m         [\n\u001b[1;32m     41\u001b[0m             T\u001b[39m.\u001b[39mRandomResize([\u001b[39m800\u001b[39m], max_size\u001b[39m=\u001b[39m\u001b[39m1333\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m         ]\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     image_source \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(image_path)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m     image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(image_source)\n\u001b[1;32m     48\u001b[0m     image_transformed, _ \u001b[39m=\u001b[39m transform(image_source, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/CalHacks2023/calHacks2023/venv/lib/python3.9/site-packages/PIL/Image.py:3243\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3240\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3242\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3243\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3244\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3246\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weights/dog-3.jpeg'"
     ]
    }
   ],
   "source": [
    "# Official GitHub implementation\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "\n",
    "IMAGE_PATH = \"weights/dog-3.jpeg\"\n",
    "TEXT_PROMPT = \"chair . person . dog .\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
